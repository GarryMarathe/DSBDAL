{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import requests\n","\n","URL = \"https://realpython.github.io/fake-jobs/\"\n","page = requests.get(URL)\n","print(page)\n","\n","print(page.text)"],"metadata":{"id":"-XFppgjmDASL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Beautiful Soup is a Python library for parsing structured data.            Import the library in your Python script and create a Beautiful Soup object.   A Beautiful Soup object that takes page.content, which is the HTML content you scraped earlier, as its input.                                                                         The second argument, \"html.parser\", makes sure that you use the appropriate parser for HTML content.\n","Note:You’ll want to pass page.content instead of page.text to avoid problems with character encoding. The .content attribute holds raw bytes, which can be decoded better than the text representation you printed earlier using the .text attribute.**"],"metadata":{"id":"9fw44higDu8U"}},{"cell_type":"markdown","source":[],"metadata":{"id":"pMj9MN4Dq_md"}},{"cell_type":"markdown","source":[],"metadata":{"id":"-lJOL-QAElKl"}},{"cell_type":"code","source":["from bs4 import BeautifulSoup\n","\n","soup = BeautifulSoup(page.content, \"html.parser\")"],"metadata":{"id":"DJeZU43GDkd2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Find Elements by ID In an HTML web page, every element can have an id attribute assigned. As the name already suggests, that id attribute makes the element uniquely identifiable on the page. You can begin to parse your page by selecting a specific element by its ID.**"],"metadata":{"id":"Co4hb1LzEmxs"}},{"cell_type":"markdown","source":["**Beautiful Soup allows you to find that specific HTML element by its ID:**"],"metadata":{"id":"mkGGwQJLE5jC"}},{"cell_type":"code","source":["results = soup.find(id=\"ResultsContainer\")"],"metadata":{"id":"36OQdSt9EoZq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**For easier viewing, you can prettify any Beautiful Soup object when you print it out. If you call .prettify() on the results variable that you just assigned above, then you’ll see all the HTML contained within the <div>:**"],"metadata":{"id":"fvnhsJH4E7jr"}},{"cell_type":"code","source":["print(results.prettify())"],"metadata":{"id":"0KryHH9UFAhl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Find Elements by HTML Class Name**"],"metadata":{"id":"EBoDU4TSFJkq"}},{"cell_type":"markdown","source":["**You’ve seen that every job posting is wrapped in a <div> element with the class card-content. Now you can work with your new object called results and select only the job postings in it.**"],"metadata":{"id":"UhClc6TdFhGW"}},{"cell_type":"code","source":["job_elements = results.find_all(\"div\", class_=\"card-content\")"],"metadata":{"id":"AE7KgzZzFl_r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, you call .find_all() on a Beautiful Soup object, which returns an iterable containing all the HTML for all the job listings displayed on that page."],"metadata":{"id":"3Kf10msVFtZL"}},{"cell_type":"code","source":["for job_element in job_elements:\n","    print(job_element, end=\"\\n\"*2)"],"metadata":{"id":"nzzPOP4_FuBs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That’s already pretty neat, but there’s still a lot of HTML! You saw earlier that your page has descriptive class names on some elements. You can pick out those child elements from each job posting with .find():\n","\n"],"metadata":{"id":"nTvMiNq-GC-j"}},{"cell_type":"code","source":["for job_element in job_elements:\n","    title_element = job_element.find(\"h2\", class_=\"title\")\n","    company_element = job_element.find(\"h3\", class_=\"company\")\n","    location_element = job_element.find(\"p\", class_=\"location\")\n","    print(title_element)\n","    print(company_element)\n","    print(location_element)\n","    print()"],"metadata":{"id":"hZ7GYRcbGGhL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Each job_element is another BeautifulSoup() object. Therefore, you can use the same methods on it as you did on its parent element, results."],"metadata":{"id":"80rgEWrSGoXU"}},{"cell_type":"markdown","source":["**Extract Text From HTML Elements**  \n","You can add .text to a Beautiful Soup object to return only the text content of the HTML elements that the object contains:"],"metadata":{"id":"_A9JQQgvGs0L"}},{"cell_type":"markdown","source":[],"metadata":{"id":"MvjbPPkeGpOj"}},{"cell_type":"code","source":["for job_element in job_elements:\n","    title_element = job_element.find(\"h2\", class_=\"title\")\n","    company_element = job_element.find(\"h3\", class_=\"company\")\n","    location_element = job_element.find(\"p\", class_=\"location\")\n","    print(title_element.text)\n","    print(company_element.text)\n","    print(location_element.text)\n","    print()"],"metadata":{"id":"0y3dnsNqGzp7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["However, it’s possible that you’ll also get some extra whitespace. Since you’re now working with Python strings, you can .strip() the superfluous whitespace. You can also apply any other familiar Python string methods to further clean up your text:\n","\n"],"metadata":{"id":"PHAEaN-CG--T"}},{"cell_type":"code","source":["for job_element in job_elements:\n","    title_element = job_element.find(\"h2\", class_=\"title\")\n","    company_element = job_element.find(\"h3\", class_=\"company\")\n","    location_element = job_element.find(\"p\", class_=\"location\")\n","    print(title_element.text.strip())\n","    print(company_element.text.strip())\n","    print(location_element.text.strip())\n","    print()"],"metadata":{"id":"gXMwcnHUG_qL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["However, you’re looking for a position as a software developer, and these results contain job postings in many other fields as well.\n","\n"],"metadata":{"id":"C3_xNnSeHMpL"}},{"cell_type":"markdown","source":["**Find Elements by Class Name and Text **\n","Not all of the job listings are developer jobs. Instead of printing out all the jobs listed on the website, you’ll first filter them using keywords.\n","\n","You know that job titles in the page are kept within h2 elements. To filter for only specific jobs, you can use the string argument:"],"metadata":{"id":"Xx_xhyBIHNQ7"}},{"cell_type":"code","source":["python_jobs = results.find_all(\"h2\", string=\"Python\")"],"metadata":{"id":"Y-I9L_2HL6Ph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"yyWMkT45MoTC"}},{"cell_type":"markdown","source":["This code finds all <h2> elements where the contained string matches \"Python\" exactly. Note that you’re directly calling the method on your first results variable. If you go ahead and print() the output of the above code snippet to your console, then you might be disappointed because it’ll be empty:"],"metadata":{"id":"x6sBuFmRMjZm"}},{"cell_type":"code","source":["print(python_jobs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gm2gtyiuL6Wj","outputId":"5c447896-16d8-4fab-8696-117b24bb30be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]}]},{"cell_type":"markdown","source":["There was a Python job in the search results, so why is it not showing up?\n","\n","When you use string= as you did above, your program looks for that string exactly. Any differences in the spelling, capitalization, or whitespace will prevent the element from matching. "],"metadata":{"id":"MpjSe7S_MpQo"}},{"cell_type":"markdown","source":["**Pass a Function to a Beautiful Soup Method**\n","In addition to strings, you can sometimes pass functions as arguments to Beautiful Soup methods. You can change the previous line of code to use a function instead:"],"metadata":{"id":"90Z6Sj7OM1V9"}},{"cell_type":"code","source":["python_jobs = results.find_all(\n","    \"h2\", string=lambda text: \"python\" in text.lower()\n",")"],"metadata":{"id":"z-pmkEQWMqcd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now you’re passing an anonymous function to the string= argument. The lambda function looks at the text of each h2 element, converts it to lowercase, and checks whether the substring \"python\" is found anywhere. You can check whether you managed to identify all the Python jobs with this approach:"],"metadata":{"id":"S6gWCL-fM8n0"}},{"cell_type":"markdown","source":[],"metadata":{"id":"CcVNC0ABNG7U"}},{"cell_type":"code","source":["print(len(python_jobs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"By3g83mLM9V8","outputId":"6eb37d8f-6bce-4c6b-ae02-75e498615411"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10\n"]}]},{"cell_type":"markdown","source":["Your program has found 10 matching job posts that include the word \"python\" in their job title!"],"metadata":{"id":"Yeof7nvbNDlt"}},{"cell_type":"code","source":["for job_element in python_jobs:\n","    title_element = job_element.find(\"h2\", class_=\"title\")\n","    company_element = job_element.find(\"h3\", class_=\"company\")\n","    location_element = job_element.find(\"p\", class_=\"location\")\n","    print(title_element.text.strip())\n","    print(company_element.text.strip())\n","    print(location_element.text.strip())\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"RnBjSy5ORiUg","outputId":"a73c182d-b3ac-4384-9f37-d8616bf6faed"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-5b918efa4e2e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcompany_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"company\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlocation_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"location\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompany_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"]}]},{"cell_type":"markdown","source":["However, when you try to run your scraper to print out the information of the filtered Python jobs, you’ll run into an error:"],"metadata":{"id":"GBvvMy4vRtm-"}},{"cell_type":"markdown","source":["**Identify Error Conditions**\n","When you look at a single element in python_jobs, you’ll see that it consists of only the <h2> element that contains the job title:\n","\n"],"metadata":{"id":"FDjunPsAR68u"}},{"cell_type":"markdown","source":["When you revisit the code you used to select the items, you’ll see that that’s what you targeted. You filtered for only the h2 title elements of the job postings that contain the word \"python\". As you can see, these elements don’t include the rest of the information about the job."],"metadata":{"id":"us1AzvNIR8PH"}},{"cell_type":"markdown","source":["You tried to find the job title, the company name, and the job’s location in each element in python_jobs, but each element contains only the job title text.\n","\n","Your diligent parsing library still looks for the other ones, too, and returns None because it can’t find them. Then, print() fails with the shown error message when you try to extract the .text attribute from one of these None objects.\n","\n","The text you’re looking for is nested in sibling elements of the h2 elements your filter returned. Beautiful Soup can help you to select sibling, child, and parent elements of each Beautiful Soup object."],"metadata":{"id":"Dc8VOInZSDd-"}},{"cell_type":"markdown","source":["**Access Parent Elements**\n","One way to get access to all the information you need is to step up in the hierarchy of the DOM starting from the h2 elements that you identified. Take another look at the HTML of a single job posting. Find the h2 element that contains the job title as well as its closest parent element that contains all the information that you’re interested in:"],"metadata":{"id":"Jbfte4bTSMlr"}},{"cell_type":"markdown","source":["The div element with the card-content class contains all the information you want. It’s a third-level parent of the h2 title element that you found using your filter.\n","\n","With this information in mind, you can now use the elements in python_jobs and fetch their great-grandparent elements instead to get access to all the information you want:"],"metadata":{"id":"i3s9CXQbS5vO"}},{"cell_type":"code","source":["python_jobs = results.find_all(\n","    \"h2\", string=lambda text: \"python\" in text.lower()\n",")\n","\n","python_job_elements = [\n","    h2_element.parent.parent.parent for h2_element in python_jobs\n","]\n","\n","\n","for job_element in python_job_elements:\n","    title_element = job_element.find(\"h2\", class_=\"title\")\n","    company_element = job_element.find(\"h3\", class_=\"company\")\n","    location_element = job_element.find(\"p\", class_=\"location\")\n","    print(title_element.text.strip())\n","    print(company_element.text.strip())\n","    print(location_element.text.strip())\n","    print()\n","    \n","    links = job_element.find_all(\"a\")\n","    for link in links:\n","        link_url = link[\"href\"]\n","        print(f\"Apply here: {link_url}\\n\")"],"metadata":{"id":"ootrxZM_S78u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ea379f7a-151a-4775-ea7a-06cb3ebb6c15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Senior Python Developer\n","Payne, Roberts and Davis\n","Stewartbury, AA\n","\n","Apply here: https://www.realpython.com\n","\n","Apply here: https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\n","\n","Software Engineer (Python)\n","Garcia PLC\n","Ericberg, AE\n","\n","Apply here: https://www.realpython.com\n","\n","Apply here: https://realpython.github.io/fake-jobs/jobs/software-engineer-python-10.html\n","\n","Python Programmer (Entry-Level)\n","Moss, Duncan and Allen\n","Port Sara, AE\n","\n","Apply here: https://www.realpython.com\n","\n","Apply here: https://realpython.github.io/fake-jobs/jobs/python-programmer-entry-level-20.html\n","\n","Python Programmer (Entry-Level)\n","Cooper and Sons\n","West Victor, AE\n","\n","Apply here: https://www.realpython.com\n","\n","Apply here: https://realpython.github.io/fake-jobs/jobs/python-programmer-entry-level-30.html\n","\n","Software Developer (Python)\n","Adams-Brewer\n","Brockburgh, AE\n","\n","Apply here: https://www.realpython.com\n","\n","Apply here: https://realpython.github.io/fake-jobs/jobs/software-developer-python-40.html\n","\n","Python Developer\n","Rivera and Sons\n","East Michaelfort, AA\n","\n","Apply here: https://www.realpython.com\n","\n","Apply here: https://realpython.github.io/fake-jobs/jobs/python-developer-50.html\n","\n","Back-End Web Developer (Python, Django)\n","Stewart-Alexander\n","South Kimberly, AA\n","\n","Apply here: https://www.realpython.com\n","\n","Apply here: https://realpython.github.io/fake-jobs/jobs/back-end-web-developer-python-django-60.html\n","\n","Back-End Web Developer (Python, Django)\n","Jackson, Ali and Mckee\n","New Elizabethside, AA\n","\n","Apply here: https://www.realpython.com\n","\n","Apply here: https://realpython.github.io/fake-jobs/jobs/back-end-web-developer-python-django-70.html\n","\n","Python Programmer (Entry-Level)\n","Mathews Inc\n","Robertborough, AP\n","\n","Apply here: https://www.realpython.com\n","\n","Apply here: https://realpython.github.io/fake-jobs/jobs/python-programmer-entry-level-80.html\n","\n","Software Developer (Python)\n","Moreno-Rodriguez\n","Martinezburgh, AE\n","\n","Apply here: https://www.realpython.com\n","\n","Apply here: https://realpython.github.io/fake-jobs/jobs/software-developer-python-90.html\n","\n"]}]},{"cell_type":"markdown","source":["When you run your script another time, you’ll see that your code once again has access to all the relevant information. That’s because you’re now looping over the div class=\"card-content\" elements instead of just the h2 title elements."],"metadata":{"id":"6uXF16LJT6r3"}},{"cell_type":"markdown","source":["You added a list comprehension that operates on each of the h2 title elements in python_jobs that you got by filtering with the lambda expression. You’re selecting the parent element of the parent element of the parent element of each h2 title element. That’s three generations up!\n","\n","When you were looking at the HTML of a single job posting, you identified that this specific parent element with the class name card-content contains all the information you need.\n","\n","Now you can adapt the code in your for loop to iterate over the parent elements instead:\n","\n"],"metadata":{"id":"G4B3-_zbTQvO"}},{"cell_type":"code","source":["for link in links:\n","        link_url = link[\"href\"]\n","        print(f\"Apply here: {link_url}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cE-a1ukLUouB","outputId":"a854e536-e893-49a9-abe1-83909b7a5967"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Apply here: https://www.realpython.com\n","\n","Apply here: https://realpython.github.io/fake-jobs/jobs/software-developer-python-90.html\n","\n"]}]},{"cell_type":"markdown","source":["In this code snippet, you first fetched all links from each of the filtered job postings. Then you extracted the href attribute, which contains the URL, using \"href\" and printed it to your console."],"metadata":{"id":"BRApgcjGUm2_"}}]}